# ----------------- VADER Analysis -----------------
def analyze_reviews_vader(reviews, timestamps=None, cleaned_reviews=None, feedback_ids=None):
    per_review_summary = []
    sentiment_counts = {"pos": 0, "neu": 0, "neg": 0}
    aspect_sentiment = {
    aspect: {
        "pos": 0,
        "neu": 0,
        "neg": 0,
        "confidence": 0.0,
        "severity_score": 0    } for aspect in ASPECT_KEYWORDS  }
    negative_words = []
    compound_scores = []
    sentiments_list = []


        # ensure feedback_ids is a list aligned with reviews (may be None)
    if feedback_ids is None:
        feedback_ids = [None] * len(reviews)


    for idx, review in enumerate(reviews):
        feedback_id = feedback_ids[idx] if idx < len(feedback_ids) else None
        text = clean_text(review)

        scores = vader_analyzer.polarity_scores(text)
        compound = scores["compound"]
        compound_scores.append(compound)

        # same label thresholds
        if compound >= 0.05:
            sentiment = "pos"
        elif compound <= -0.05:
            sentiment = "neg"
        else:
            sentiment = "neu"
        sentiments_list.append(sentiment)
        sentiment_counts[sentiment] += 1

        # calibration: blend compound + pos/neg strengths + punctuation/emoji bonuses
        pos_s = scores.get("pos", 0.0)
        neg_s = scores.get("neg", 0.0)
        strength = max(pos_s, neg_s)  # 0..1

        punct_count = min(3, text.count("!") + text.count("?"))
        punct_bonus = punct_count * 0.03       # up to +0.09

        emoji_count = sum(1 for ch in text if ord(ch) > 127)   # quick emoji / non-ascii heuristic
        emoji_bonus = min(0.06, emoji_count * 0.02)            # up to +0.06

        calibrated = (abs(compound) * 0.6) + (strength * 0.3) + punct_bonus + emoji_bonus
        calibrated = max(0.0, min(1.0, calibrated))
        confidence_pct = int(round(calibrated * 100))
        vpos = scores.get("pos", 0.0)
        vneg = scores.get("neg", 0.0)
        vneu = max(0.0, 1.0 - (vpos + vneg))
        total_p = vpos + vneg + vneu
        vpos, vneg, vneu = vpos/total_p, vneg/total_p, vneu/total_p

        per_review_summary.append({
             "id": idx + 1,
            "feedback_id": feedback_id,
            "text": review[:50] + ("..." if len(review) > 50 else ""),
            "sentiment": sentiment,
            "compound": compound,
            "confidence": confidence_pct,
            "vader_pos_pct": round(vpos * 100, 2),
            "vader_neg_pct": round(vneg * 100, 2),
            "vader_neu_pct": round(vneu * 100, 2)
        })



    total = len(reviews) if reviews else 1
    sentiment_distribution = {
        "pos": sentiment_counts["pos"],
        "neg": sentiment_counts["neg"],
        "neu": sentiment_counts["neu"],
        "pos_percent": round(sentiment_counts["pos"] / total * 100, 2),
        "neg_percent": round(sentiment_counts["neg"] / total * 100, 2),
        "neu_percent": round(sentiment_counts["neu"] / total * 100, 2)
    }

    # compute severity = % negative for each aspect
    for asp, stats in aspect_sentiment.items():
        total = stats["pos"] + stats["neu"] + stats["neg"]
        if total > 0:
            stats["severity_score"] = int((stats["neg"] / total) * 100)
        else:
            stats["severity_score"] = 0
    avg_abs_comp = float(np.mean(np.abs(np.array(compound_scores)))) if compound_scores else 0

    for asp, stats in aspect_sentiment.items():
        total = stats["pos"] + stats["neu"] + stats["neg"]
        mention_factor = min(1.0, total / 3)
        stats["confidence"] = round(0.5 * mention_factor + 0.5 * avg_abs_comp, 4)



    top_issues_counter = Counter(negative_words)
    total_negatives = sum(top_issues_counter.values()) or 1
    top_issues = [
        {"keyword": kw, "count": cnt, "percent": round((cnt / total_negatives) * 100, 2)}
        for kw, cnt in top_issues_counter.most_common(TOP_N_ISSUES)
    ]

    denom = sum([it["count"] for it in top_issues]) or 1
    for it in top_issues:
        # human-friendly label (if cluster present it's already readable)
        it["label"] = it.get("keyword", it.get("issue", "")).replace("_", " ").title()
        # severity: % scaled 0-100
        it["severity"] = int(round(it.get("percent", 0)))
        # impact: simple weighted metric (tunable)
        it["impact"] = round(it.get("percent", 0) * (1 + (it.get("count", 0) / max(1, denom))) , 2)
        # representative keywords: pick top tokens belonging to this cluster
        it["keywords"] = []
        for cluster, kws in ISSUE_CLUSTERS.items():
            if cluster == it["keyword"]:
                it["keywords"] = kws[:4]
                break
        # example review: first review that contains any keyword from keywords list
        it["example"] = "No sample available."
        for rv in reviews:
            rv_low = rv.lower()
            for kw in it["keywords"]:
                if kw in rv_low:
                    it["example"] = (rv[:140] + "...") if len(rv) > 140 else rv
                    break
            if it["example"] != "No sample available.":
                break
        # small synthetic trend array (3 points) for UI sparklines - can be replaced with real timeseries later
        it["trend"] = [max(0, round(it["percent"] * f, 2)) for f in [0.6, 0.9, 1.0]]

    suggestions = generate_suggestions(top_issues,
                                   sentiment_distribution=sentiment_distribution,
                                   compound_scores=compound_scores,
                                   aspect_sentiment=aspect_sentiment,
                                   top_k=5)
    impact_score = compute_impact_score(sentiment_distribution, top_issues)
    confidence_overview = compute_confidence_stats(compound_scores)
    wordcloud_data = generate_wordcloud_data(reviews, sentiments_list)
    trend_over_time = compute_trends(reviews, timestamps, sentiments_list)

        # prepare CSV with FeedbackID and global distribution included per-row
    output_csv = io.StringIO()
    writer = csv.writer(output_csv)
    # Header: keep helpful fields and global distribution
    writer.writerow([
        "Review ID", "FeedbackID", "Text", "Sentiment", "Compound", "Confidence(%)",
        "Pos% (global)", "Neg% (global)", "Neu% (global)"
    ])

    # For mapping cleaned_reviews (if provided) use them; else use original text snippets
    cleaned_iter = cleaned_reviews if cleaned_reviews is not None else [r for r in reviews]

    for rev, clean_text_value in zip(per_review_summary, cleaned_iter):
        # conf_pct = round(abs(rev.get("compound", 0)) * 100, 2)
        conf_pct = rev.get("confidence", 0)
        writer.writerow([
            rev.get("id"),
            rev.get("feedback_id"),
            (clean_text_value[:140] + "...") if len(clean_text_value) > 140 else clean_text_value,
            rev.get("sentiment"),
            rev.get("compound"),
            conf_pct,
            rev.get("vader_pos_pct", sentiment_distribution.get("pos_percent", 0)),
            rev.get("vader_neg_pct", sentiment_distribution.get("neg_percent", 0)),
            rev.get("vader_neu_pct", sentiment_distribution.get("neu_percent", 0))
        ])

    report_csv_base64 = output_csv.getvalue()



    return {
        "sentiment_distribution": sentiment_distribution,
        "per_review_summary": per_review_summary,
        "aspect_sentiment": aspect_sentiment,
        "top_issues": top_issues,
        "suggestions": suggestions,
        "impact_score": impact_score,
        "confidence_overview": confidence_overview,
        "wordcloud_data": wordcloud_data,
        "trend_over_time": trend_over_time,
        "report_download": report_csv_base64
    }

